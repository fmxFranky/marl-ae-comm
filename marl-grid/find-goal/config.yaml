env_cfg:
  active_after_done: false
  can_overlap: false
  clutter_density: 0.15
  comm_len: 10
  # if False, use continuous communication
  discrete_comm: true
  discrete_position: true
  env_name: null
  env_type: c
  grid_size: 15
  info_gain_rew: false
  max_steps: 512
  neutral_shape: true
  num_adversaries: 0
  num_agents: 3
  # if `num_blind_agents` == b, the FIRST b agents do not get image obs
  num_blind_agents: 0
  observation_style: dict
  observe_done: false
  observe_position: false
  observe_self_env_act: false
  observe_self_position: true
  observe_t: false
  seed: ${seed}
  # update env policy / comm policy using only env reward / team reward
  separate_rew_more: false
  team_reward_freq: none
  team_reward_multiplier: 1
  team_reward_type: share
  view_size: 7
  view_tile_size: 6

# async update steps
tmax: 20

# max total training iterations
train_iter: 300000
lr: 0.0001

# experiment id
resume_path: ""

# the policy head
policy: lstm
model: shared
share_critic: false
layer_norm: true
comm_rnn: true

# mask logits of unavailable actions
mask: true

# # update env policy using only env reward
# separate_rew: true

# training
anneal_comm_rew: false
ae_loss_k: 1.0
ae_std: 1.0
ae_pg: 0
ae_type: "" # ['', 'fc', 'mlp', 'rfc', 'rmlp']
ae_agent_done: false
img_feat_dim: 64
comm_vf: false

# auxilary task
aux_agents: ${env_cfg.num_agents}
aux_bsz: 128
add_auxiliary_loss: true
aux_length: 16
aux_loss_k: 1.0
aux_rb_size: 100000
num_attn_heads: 8
num_attn_layers: 2
momentum_update_freq: 2
momentum_tau: 0.05
aux_lr: 0.00005

# eval configs
video_save_freq: 20
ckpt_save_freq: 100
num_eval_episodes: 10
num_eval_videos: 10
eval_ae: false

# experiment configs
algo: ae-comm # ['no-comm', 'rl-comm', 'ae-rl-comm', 'ae-comm']
gpu:
  - 0
id: ""
exp_name: null
seed: 1
num_workers: 4
run_dir: ./runs
use_wandb: false
wandb_project_name: marl-ae-jpr-comm
